---
layout: archive
title: "News"
permalink: /news/
author_profile: true
---
<style>
.news-section {
  line-height: 1.8;
  color: #333;
}
.news-section h2 {
  color: #2c3e50;
  border-bottom: 2px solid #3498db;
  padding-bottom: 8px;
  margin-top: 2em;
}
.news-item {
  margin-bottom: 2em;
}
.news-date {
  font-size: 1.1em;
  color: #2c3e50;
  margin-bottom: 0.5em;
}
.news-item ul {
  margin-top: 0.8em;
  margin-bottom: 1.5em;
}
.news-item li {
  margin-bottom: 1em;
  line-height: 1.7;
}
.news-item strong {
  color: #2c3e50;
}
.news-item a {
  color: #3498db;
  text-decoration: none;
  border-bottom: 1px solid transparent;
  transition: border-bottom 0.2s ease;
}
.news-item a:hover {
  border-bottom: 1px solid #3498db;
}
.news-footer {
  color: #7f8c8d;
  text-align: center;
  margin-top: 3em;
  padding-top: 2em;
  border-top: 1px solid #ecf0f1;
}
</style>
<div class="news-section">
<h2>2025</h2>
<div class="news-item">
<p class="news-date">October 2025</p>
ðŸŽ‰ Two exciting papers archived on arXiv this month!
<ul>
<li><strong>Frequency-Aware Model Parameter Explorer</strong> - Introducing a novel attribution method for improving explainability in AI models. This work represents a significant step forward in understanding how neural networks process information through frequency-domain analysis. <a href="https://arxiv.org/abs/2510.03245v1">Read the paper â†’</a></li>
<li><strong>Survival at Any Cost?</strong> - Our paper explores critical questions about LLM decision-making under pressure and moral trade-offs. <a href="https://arxiv.org/abs/2509.12190">Read the paper â†’</a></li>
</ul>
</div>
<div class="news-item">
<p class="news-date">January 2025</p>
ðŸš€ Started as Research Intern at ZEISS Lab Ã— Medical University of Vienna, working remotely with an international team on frequency-based explainability methods for AI systems.
</div>
<h2>2024</h2>
<div class="news-item">
<p class="news-date">September 2024</p>
ðŸ“Š Reached 40+ citations on Google Scholar with h-index of 4. Grateful for the research community's engagement with our work!
</div>
<p class="news-footer">Stay tuned for more updates on AI safety, explainability, and alignment research!</p>
</div>
